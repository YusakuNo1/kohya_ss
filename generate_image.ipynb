{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"peft==0.13.1\" \"diffusers==0.26.0\" \"huggingface_hub==0.24.0\" \"diffusers==0.26.0\" transformers accelerate\n",
    "%pip install peft==0.14.0 diffusers==0.32.2 transformers==4.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc263ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ 5090 æ­£åœ¨åŠ è½½å…¨é‡è‡ªå®šä¹‰æ¨¡å‹ (SDXL)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 17 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 224931.13it/s]\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ 5090 æ­£åœ¨æ¨ç†...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:02<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸï¼å…¨é‡æ¨¡å‹ç”Ÿæˆçš„å›¾ç‰‡å·²ä¿å­˜: liying_zhao_5090_test_full_tune.png\n"
     ]
    }
   ],
   "source": [
    "# Full tune ç”Ÿæˆå›¾ç‰‡ç¤ºä¾‹ä»£ç \n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# è·¯å¾„æŒ‡å‘ä½ é‚£ä¸ª 6G çš„æ–‡ä»¶\n",
    "model_path = \"./outputs/new model.safetensors\"\n",
    "output_name = \"liying_zhao_5090_test_full_tune.png\"\n",
    "\n",
    "print(\"ğŸš€ 5090 æ­£åœ¨åŠ è½½å…¨é‡è‡ªå®šä¹‰æ¨¡å‹ (SDXL)...\")\n",
    "\n",
    "# æ³¨æ„ï¼šè¿™é‡Œæ”¹ç”¨ from_single_file æ–¹æ³•åŠ è½½ 6G çš„ safetensors\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "# prompt = \"A high-quality portrait of EWOLiyingZhao, 8k\"\n",
    "prompt = \"EWOLiyingZhao with long hair, wearing a blue shirt. She is posing for the camera with her hands on her cheeks, giving off a playful and confident vibe. The woman appears to be smiling, possibly for the camera. The background of the image is blurry, which helps to focus on the subject.\"\n",
    "\n",
    "print(\"âš¡ 5090 æ­£åœ¨æ¨ç†...\")\n",
    "with torch.inference_mode():\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "        width=1024,\n",
    "        height=1024\n",
    "    ).images[0]\n",
    "\n",
    "image.save(output_name)\n",
    "print(f\"âœ… æˆåŠŸï¼å…¨é‡æ¨¡å‹ç”Ÿæˆçš„å›¾ç‰‡å·²ä¿å­˜: {output_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33fb05d",
   "metadata": {},
   "source": [
    "# Lora ç”Ÿæˆå›¾ç‰‡ç¤ºä¾‹ä»£ç  with SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bdd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yusaku/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-19 20:05:32.228870: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-19 20:05:32.632041: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-19 20:05:32.632125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-19 20:05:32.708688: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-19 20:05:32.840477: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-19 20:05:33.758948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° LoRA æ–‡ä»¶ï¼è¯·ç¡®è®¤ last.safetensors çš„ä½ç½®ã€‚\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# --- 1. è‡ªåŠ¨è·¯å¾„çº é”™é€»è¾‘ ---\n",
    "def get_lora_path():\n",
    "    candidates = [\n",
    "        \"./outputs/SD15_LiyingZhao_v1.safetensors\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "LORA_PATH = get_lora_path()\n",
    "BASE_MODEL = \"./models/sd_xl_base_1.0.safetensors\"\n",
    "TRIGGER_WORD = \"EWOLiyingZhao\"\n",
    "# PROMPT = f\"A high-quality portrait of {TRIGGER_WORD}, traditional Chinese dress, cinematic lighting, 8k\"\n",
    "PROMPT = f\"{TRIGGER_WORD} with long hair, wearing a blue shirt. She is posing for the camera with her hands on her cheeks, giving off a playful and confident vibe. The woman appears to be smiling, possibly for the camera. The background of the image is blurry, which helps to focus on the subject.\"\n",
    "OUTPUT_NAME = \"liying_zhao_5090_test_lora.png\"\n",
    "\n",
    "def run_inference():\n",
    "    if not LORA_PATH:\n",
    "        print(\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° LoRA æ–‡ä»¶ï¼è¯·ç¡®è®¤ last.safetensors çš„ä½ç½®ã€‚\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸš€ æ‰¾åˆ°æƒé‡: {LORA_PATH}\")\n",
    "    print(\"â³ 5090 åŠ è½½æ¨¡å‹ä¸­ (BFloat16)...\")\n",
    "    \n",
    "    # é’ˆå¯¹ 5090 ä¼˜åŒ–\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_safetensors=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    print(f\"ğŸ¨ æ³¨å…¥ LoRA...\")\n",
    "    # å¦‚æœè¿™è¡ŒæŠ¥é”™ï¼Œè¯·å…ˆæ‰§è¡Œ pip install peft\n",
    "    pipe.load_lora_weights(LORA_PATH)\n",
    "\n",
    "    print(\"âš¡ 5090 Blackwell æ¶æ„å…¨é€Ÿå‡ºå›¾ä¸­...\")\n",
    "    with torch.inference_mode():\n",
    "        image = pipe(\n",
    "            prompt=PROMPT,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=7.5,\n",
    "            width=1024,\n",
    "            height=1024\n",
    "        ).images[0]\n",
    "\n",
    "    image.save(OUTPUT_NAME)\n",
    "    print(f\"âœ… æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: {OUTPUT_NAME}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913df5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ 5090 åŠ è½½ SD 1.5 åº•æ¨¡...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ æ³¨å…¥ SD 1.5 LoRA...\n",
      "âš¡ 5090 æé€Ÿç”Ÿæˆä¸­ (SD 1.5 åœ¨ 5090 ä¸Šé€šå¸¸åªéœ€ 1 ç§’)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 31.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: /home/yusaku/Workspace/äººè„¸FineTuneWorkspace/kohya_ss/liying_zhao_sd15_test.png\n"
     ]
    }
   ],
   "source": [
    "# SD 1.5 LoRA ç”Ÿæˆå›¾ç‰‡ç¤ºä¾‹ä»£ç \n",
    "\n",
    "import torch\n",
    "import os\n",
    "from diffusers import StableDiffusionPipeline  # ä¿®æ­£ï¼šä» XL æ¢å› 1.5 å¯¹åº”çš„ Pipeline\n",
    "\n",
    "# --- 1. é…ç½®è·¯å¾„ ---\n",
    "LORA_PATH = \"./outputs/SD15_LiyingZhao_v1.safetensors\"\n",
    "# ä¿®æ­£ï¼šä½¿ç”¨ SD 1.5 å®˜æ–¹æ¨¡å‹ä½œä¸ºåº•æ¨¡ï¼Œä¸ LoRA åŒ¹é…\n",
    "BASE_MODEL = \"runwayml/stable-diffusion-v1-5\" \n",
    "TRIGGER_WORD = \"EWOLiyingZhao\"\n",
    "PROMPT = f\"A photo of {TRIGGER_WORD}, wearing a blue shirt, smiling, 8k resolution, highly detailed.\"\n",
    "OUTPUT_NAME = \"liying_zhao_sd15_test.png\"\n",
    "\n",
    "def run_inference():\n",
    "    if not os.path.exists(LORA_PATH):\n",
    "        print(\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° LoRA æ–‡ä»¶ï¼\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸš€ 5090 åŠ è½½ SD 1.5 åº•æ¨¡...\")\n",
    "    \n",
    "    # åŠ è½½ SD 1.5 ç®¡é“\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float16, # SD 1.5 é€šå¸¸ç”¨ float16 å°±å¾ˆç¨³\n",
    "        use_safetensors=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    print(f\"ğŸ¨ æ³¨å…¥ SD 1.5 LoRA...\")\n",
    "    # æ³¨å…¥åˆšæ‰è®­ç»ƒå¥½çš„ 512 åˆ†è¾¨ç‡ LoRA\n",
    "    pipe.load_lora_weights(LORA_PATH)\n",
    "\n",
    "    print(\"âš¡ 5090 æé€Ÿç”Ÿæˆä¸­ (SD 1.5 åœ¨ 5090 ä¸Šé€šå¸¸åªéœ€ 1 ç§’)...\")\n",
    "    with torch.inference_mode():\n",
    "        image = pipe(\n",
    "            prompt=PROMPT,\n",
    "            negative_prompt=\"(worst quality, low quality:1.4), deformed, blurry\",\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=7.5,\n",
    "            width=512,  # ä¿®æ­£ï¼šSD 1.5 å»ºè®®ç”¨ 512\n",
    "            height=512\n",
    "        ).images[0]\n",
    "\n",
    "    image.save(OUTPUT_NAME)\n",
    "    print(f\"âœ… æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_NAME)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66825a34",
   "metadata": {},
   "source": [
    "# LoRAç”Ÿæˆå›¾ç‰‡with DreamBooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6d1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 17 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 28716.54it/s]\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ æ­£åœ¨ç”Ÿæˆå›¾ç‰‡...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_6.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_7.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_8.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º result_sdxl_9.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# 1. è·¯å¾„è®¾ç½®\n",
    "model_path = \"./outputs/SDXL_DB_LYZ_Resume.ckpt\" # ç¡®ä¿åç¼€æ­£ç¡®\n",
    "\n",
    "# 2. åˆå§‹åŒ– Pipeline\n",
    "# æˆ‘ä»¬ç›´æ¥åŠ è½½å•æ–‡ä»¶æƒé‡ (single_file_load)\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16, \n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# 3. é’ˆå¯¹ 5090 å¼€å¯åŠ é€Ÿ\n",
    "pipe.to(\"cuda\")\n",
    "# 5090 æ”¯æŒé«˜æ•ˆçš„ç®—å­åŠ é€Ÿ\n",
    "# pipe.enable_xformers_memory_efficient_attention() # å¦‚æœä½ å®‰è£…äº† xformers\n",
    "pipe.enable_attention_slicing() \n",
    "\n",
    "# 4. è®¾ç½® Prompt\n",
    "# è®°å¾—åŠ å…¥ä½ åœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„è§¦å‘è¯ \"EWOLiyingZhao\"\n",
    "prompt = \"A high-quality portrait of EWOLiyingZhao, a beautiful woman smiling, highly detailed, 8k wallpaper, cinematic lighting\"\n",
    "negative_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality\"\n",
    "\n",
    "# 5. æ‰§è¡Œç”Ÿæˆ\n",
    "print(\"ğŸš€ æ­£åœ¨ç”Ÿæˆå›¾ç‰‡...\")\n",
    "# ä¸€æ¬¡ç”Ÿæˆ10å¼ å›¾ç‰‡\n",
    "N = 10\n",
    "for i in range(N):\n",
    "    with torch.inference_mode():\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=30, # SDXL å»ºè®® 30-40 æ­¥\n",
    "            guidance_scale=7.5,\n",
    "            width=1024,\n",
    "            height=1024\n",
    "        ).images[0]\n",
    "\n",
    "    # 6. ä¿å­˜ç»“æœ\n",
    "    file_name = f\"result_sdxl_{i}.png\"\n",
    "    image.save(file_name)\n",
    "    print(f\"âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ad855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use as LoRA\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# é…ç½®æ–‡ä»¶è·¯å¾„\n",
    "lora_path = \"./outputs/SDXL_DB_LYZ_Resume_Extracted_LoRA.safetensors\"\n",
    "models = {\n",
    "    \"PonyV6\": \"./models/AstraliteHeart-pony-diffusion-v6.safetensors\",\n",
    "}\n",
    "\n",
    "# é’ˆå¯¹ 5090 çš„é€šç”¨ Prompt é…ç½®\n",
    "# æ³¨æ„ï¼šPony æ¨¡å‹é€šå¸¸éœ€è¦ç‰¹å®šçš„èµ·æ‰‹è¯æ‰èƒ½è¾¾åˆ°æœ€ä½³æ•ˆæœ\n",
    "prompt_map = {\n",
    "    \"PonyV6\": \"score_9, score_8_up, score_7_up, portrait of EWOLiyingZhao, beautiful woman, high quality, cinematic lighting\",\n",
    "}\n",
    "\n",
    "negative_prompt = \"lowres, bad anatomy, bad hands, text, error, blurry, deformed, watermark\"\n",
    "\n",
    "for name, model_path in models.items():\n",
    "    print(f\"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {name}...\")\n",
    "    \n",
    "    # 1. åŠ è½½åº•æ¨¡\n",
    "    pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "        model_path, \n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # 2. æŒ‚è½½æå–å‡ºæ¥çš„ LoRA\n",
    "    # adapter_name å¯ä»¥éšæ„å‘½åï¼Œç”¨äºåŒºåˆ†\n",
    "    pipe.load_lora_weights(lora_path, adapter_name=\"lyz\")\n",
    "    \n",
    "    # 3. é’ˆå¯¹ 5090 ä¼˜åŒ–æ¨ç†\n",
    "    pipe.enable_attention_slicing()\n",
    "    \n",
    "    print(f\"ğŸ¨ æ­£åœ¨ä½¿ç”¨ {name} ç”Ÿæˆå›¾ç‰‡...\")\n",
    "    with torch.inference_mode():\n",
    "        image = pipe(\n",
    "            prompt=prompt_map[name],\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=35,\n",
    "            guidance_scale=5.0, # Pony å»ºè®®é™ä½ CFG\n",
    "            width=1024,\n",
    "            height=1024\n",
    "        ).images[0]\n",
    "    \n",
    "    # 4. ä¿å­˜ç»“æœ\n",
    "    output_filename = f\"result_{name}_with_lora.png\"\n",
    "    image.save(output_filename)\n",
    "    print(f\"âœ… å·²ä¿å­˜: {output_filename}\")\n",
    "    \n",
    "    # 5. æ¸…ç†æ˜¾å­˜ä»¥ä¾¿åŠ è½½ä¸‹ä¸€ä¸ªæ¨¡å‹\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nâœ¨ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼è¯·æ£€æŸ¥å½“å‰ç›®å½•ä¸‹çš„ png æ–‡ä»¶ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92525bec",
   "metadata": {},
   "source": [
    "# Lora ç”Ÿæˆå›¾ç‰‡ç¤ºä¾‹ä»£ç  with Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d6ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ 5090 è½½å…¥ LoRA: ./outputs/EWOLiyingZhao_Flux_v2_1024.safetensors\n",
      "â³ åŠ è½½ Flux åŸºç¡€æ¨¡å‹ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.28it/s]it/s]\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ æ³¨å…¥ LoRA æƒé‡...\n",
      "âš¡ 5090 å…¨é€Ÿå‡ºå›¾ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [04:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(OUTPUT_NAME)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš¡ 5090 å…¨é€Ÿå‡ºå›¾ä¸­...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Flux é»˜è®¤å‚æ•°ï¼š1024x1024, 20-30 steps, guidance 3.5\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# T5 å¤„ç†é•¿åº¦\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m image\u001b[38;5;241m.\u001b[39msave(OUTPUT_NAME)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(OUTPUT_NAME)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/diffusers/pipelines/flux/pipeline_flux.py:889\u001b[0m, in \u001b[0;36mFluxPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, negative_prompt, negative_prompt_2, true_cfg_scale, height, width, num_inference_steps, sigmas, guidance_scale, num_images_per_prompt, generator, latents, prompt_embeds, pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, negative_ip_adapter_image, negative_ip_adapter_image_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, joint_attention_kwargs, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[1;32m    887\u001b[0m timestep \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mexpand(latents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(latents\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 889\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtxt_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_image_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_true_cfg:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m negative_image_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/diffusers/models/transformers/transformer_flux.py:565\u001b[0m, in \u001b[0;36mFluxTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, pooled_projections, timestep, img_ids, txt_ids, guidance, joint_attention_kwargs, controlnet_block_samples, controlnet_single_block_samples, return_dict, controlnet_blocks_repeat)\u001b[0m\n\u001b[1;32m    556\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    557\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    558\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# controlnet residual\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m controlnet_single_block_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/diffusers/models/transformers/transformer_flux.py:97\u001b[0m, in \u001b[0;36mFluxSingleTransformerBlock.forward\u001b[0;34m(self, hidden_states, temb, image_rotary_emb, joint_attention_kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m mlp_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_mlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_mlp(norm_hidden_states))\n\u001b[1;32m     96\u001b[0m joint_attention_kwargs \u001b[38;5;241m=\u001b[39m joint_attention_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 97\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([attn_output, mlp_hidden_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    104\u001b[0m gate \u001b[38;5;241m=\u001b[39m gate\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/diffusers/models/attention_processor.py:588\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    586\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2319\u001b[0m, in \u001b[0;36mFluxAttnProcessor2_0.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, image_rotary_emb)\u001b[0m\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_rotary_emb\n\u001b[1;32m   2318\u001b[0m     query \u001b[38;5;241m=\u001b[39m apply_rotary_emb(query, image_rotary_emb)\n\u001b[0;32m-> 2319\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2321\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m   2322\u001b[0m     query, key, value, attn_mask\u001b[38;5;241m=\u001b[39mattention_mask, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2323\u001b[0m )\n\u001b[1;32m   2324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m head_dim)\n",
      "File \u001b[0;32m~/miniconda3/envs/kohya_ss/lib/python3.10/site-packages/diffusers/models/embeddings.py:1208\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(x, freqs_cis, use_real, use_real_unbind_dim)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`use_real_unbind_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_real_unbind_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` but should be -1 or -2.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1208\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_rotated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# used for lumina\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# from diffusers import FluxPipeline\n",
    "\n",
    "# # --- 1. é…ç½®è·¯å¾„ ---\n",
    "# # ç¡®ä¿è¿™æ˜¯ä½ è®­ç»ƒè„šæœ¬è¾“å‡ºçš„ .safetensors æ–‡ä»¶è·¯å¾„\n",
    "# LORA_PATH = \"./outputs/EWOLiyingZhao_Flux_v2_1024.safetensors\"\n",
    "# BASE_MODEL = \"./models/FLUX.1-dev\"\n",
    "# TRIGGER_WORD = \"EWOLiyingZhao\"\n",
    "\n",
    "# # å»ºè®® Prompt åŒ…å«è®­ç»ƒæ—¶çš„è§¦å‘è¯\n",
    "# PROMPT = f\"{TRIGGER_WORD} with long hair, wearing a blue shirt, smiling, 8k resolution, cinematic lighting, masterpiece.\"\n",
    "# OUTPUT_NAME = \"liying_zhao_flux_final_test.png\"\n",
    "\n",
    "# def run_inference():\n",
    "#     if not os.path.exists(LORA_PATH):\n",
    "#         print(f\"âŒ é”™è¯¯ï¼šåœ¨ ./outputs/ æ²¡æ‰¾åˆ° LoRA æ–‡ä»¶ï¼\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"ğŸš€ 5090 è½½å…¥ LoRA: {LORA_PATH}\")\n",
    "#     print(f\"â³ åŠ è½½ Flux åŸºç¡€æ¨¡å‹ä¸­...\")\n",
    "\n",
    "#     # åŠ è½½ Flux ç®¡é“\n",
    "#     pipe = FluxPipeline.from_pretrained(\n",
    "#         BASE_MODEL,\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         use_safetensors=True\n",
    "#     )\n",
    "\n",
    "#     # ğŸ› ï¸ é’ˆå¯¹ 5090 å’Œ 64G å†…å­˜çš„ä¼˜åŒ–ç­–ç•¥\n",
    "#     # è¿™ä¸€è¡Œä¼šè‡ªåŠ¨å¹³è¡¡ T5 ç¼–ç å™¨å’Œ Transformer çš„æ˜¾å­˜å ç”¨ï¼Œå½»åº•å‘Šåˆ«å¡é¡¿\n",
    "#     pipe.enable_model_cpu_offload()\n",
    "\n",
    "#     print(f\"ğŸ¨ æ³¨å…¥ LoRA æƒé‡...\")\n",
    "#     pipe.load_lora_weights(LORA_PATH)\n",
    "\n",
    "#     print(\"âš¡ 5090 å…¨é€Ÿå‡ºå›¾ä¸­...\")\n",
    "    \n",
    "#     with torch.inference_mode():\n",
    "#         # Flux é»˜è®¤å‚æ•°ï¼š1024x1024, 20-30 steps, guidance 3.5\n",
    "#         image = pipe(\n",
    "#             prompt=PROMPT,\n",
    "#             height=1024,\n",
    "#             width=1024,\n",
    "#             guidance_scale=3.5, \n",
    "#             num_inference_steps=28,\n",
    "#             max_sequence_length=512 # T5 å¤„ç†é•¿åº¦\n",
    "#         ).images[0]\n",
    "\n",
    "#     image.save(OUTPUT_NAME)\n",
    "#     print(f\"âœ… æˆåŠŸï¼å›¾ç‰‡å·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_NAME)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51106a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ 5090 æ­£åœ¨æ„å»º Flux ç®¡é“...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.19it/s]it/s]\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ æ³¨å…¥æ–°è®­ç»ƒçš„ LoRA...\n",
      "âš¡ å¯åŠ¨é¦–æ­¥æ¨ç† (ç”±äºé¦–æ­¥æ¶‰åŠç®—å­åˆå§‹åŒ–ï¼Œè¯·è€å¿ƒç­‰å¾… 30-60 ç§’)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:47<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸï¼å›¾ç‰‡ä¿å­˜åœ¨: /home/yusaku/Workspace/äººè„¸FineTuneWorkspace/kohya_ss/liying_zhao_v2_test.png\n"
     ]
    }
   ],
   "source": [
    "# 5090 Flux LoRA æ¨ç†ç¤ºä¾‹ä»£ç ï¼ˆé’ˆå¯¹ 32GB æ˜¾å¡ä¼˜åŒ–ï¼‰\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "# --- 1. å½»åº•å†·å¯åŠ¨æ¸…ç† ---\n",
    "def flush_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect() # æ˜¾å¼æ¸…ç†è¿›ç¨‹é—´é€šä¿¡æ®‹ç•™\n",
    "\n",
    "flush_memory()\n",
    "\n",
    "LORA_PATH = \"./outputs/EWOLiyingZhao_Flux_Final_v7.safetensors\"\n",
    "BASE_MODEL = \"./models/FLUX.1-dev\"\n",
    "TRIGGER_WORD = \"EWOLiyingZhao\"\n",
    "PROMPT = f\"{TRIGGER_WORD} with long hair, wearing a blue shirt, smiling, 8k resolution.\"\n",
    "OUTPUT_NAME = \"liying_zhao_v2_test.png\"\n",
    "\n",
    "def run_inference():\n",
    "    if not os.path.exists(LORA_PATH):\n",
    "        print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {LORA_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"â³ 5090 æ­£åœ¨æ„å»º Flux ç®¡é“...\")\n",
    "    \n",
    "    # é’ˆå¯¹ 5090 ä¼˜åŒ–ï¼šä½¿ç”¨ low_cpu_mem_usage é™ä½åŠ è½½è´Ÿè·\n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_safetensors=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    # é’ˆå¯¹ 32GB æ˜¾å¡çš„é»„é‡‘æ³•åˆ™\n",
    "    pipe.enable_model_cpu_offload()\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ¨ æ³¨å…¥æ–°è®­ç»ƒçš„ LoRA...\")\n",
    "\n",
    "    # å¢åŠ  adapter_name æ˜ç¡®æ³¨å…¥è·¯å¾„\n",
    "    pipe.load_lora_weights(LORA_PATH)\n",
    "\n",
    "    # # å°†å…¶æ›¿æ¢ä¸ºæ‰‹åŠ¨æ§åˆ¶å¼ºåº¦çš„é€»è¾‘\n",
    "    # pipe.load_lora_weights(LORA_PATH, adapter_name=\"yusaku_lora\")\n",
    "    # # å°†å¼ºåº¦ä»é»˜è®¤çš„ 1.0 é™ä½åˆ° 0.5 ç”šè‡³ 0.1\n",
    "    # pipe.set_adapters([\"yusaku_lora\"], adapter_weights=[0.3])\n",
    "\n",
    "\n",
    "\n",
    "    print(\"âš¡ å¯åŠ¨é¦–æ­¥æ¨ç† (ç”±äºé¦–æ­¥æ¶‰åŠç®—å­åˆå§‹åŒ–ï¼Œè¯·è€å¿ƒç­‰å¾… 30-60 ç§’)...\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        image = pipe(\n",
    "            prompt=PROMPT,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            guidance_scale=3.5, \n",
    "            num_inference_steps=28,\n",
    "            max_sequence_length=512\n",
    "        ).images[0]\n",
    "\n",
    "    image.save(OUTPUT_NAME)\n",
    "    print(f\"âœ… æˆåŠŸï¼å›¾ç‰‡ä¿å­˜åœ¨: {os.path.abspath(OUTPUT_NAME)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kohya_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
